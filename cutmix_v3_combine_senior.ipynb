{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class index\n",
    "CLASS_INDEX = {\n",
    "    'hat_on': 0,\n",
    "    'hat_off': 1,\n",
    "    'clothes_on': 2,\n",
    "    'clothes_off': 3,\n",
    "    'shoes_on': 4,\n",
    "    'shoes_off': 5,\n",
    "    'mask_on': 6,\n",
    "    'mask_off': 7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[1]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_labels(labels_a, labels_b, bbx1, bby1, bbx2, bby2, image_size):\n",
    "    new_labels = []\n",
    "\n",
    "    for label in labels_a:\n",
    "        class_index, x_center, y_center, width, height = label\n",
    "        x_center *= image_size[2]\n",
    "        y_center *= image_size[1]\n",
    "        width *= image_size[2]\n",
    "        height *= image_size[1]\n",
    "\n",
    "        if bbx1 <= x_center <= bbx2 and bby1 <= y_center <= bby2:\n",
    "            continue # Skip covered labels\n",
    "\n",
    "        new_x_center = x_center / image_size[2]\n",
    "        new_y_center = y_center / image_size[1]\n",
    "        new_width = width / image_size[2]\n",
    "        new_height = height / image_size[1]\n",
    "        new_labels.append([class_index, new_x_center, new_y_center, new_width, new_height])\n",
    "    \n",
    "    for label in labels_b:\n",
    "        class_index, x_center, y_center, width, height = label\n",
    "        x_center *= image_size[2]\n",
    "        y_center *= image_size[1]\n",
    "        width *= image_size[2]\n",
    "        height *= image_size[1]\n",
    "\n",
    "        if not (bbx1 <= x_center <= bbx2 and bby1 <= y_center <= bby2):\n",
    "            continue # Skip uncovered labels\n",
    "\n",
    "        new_x_center = (x_center - bbx1) / image_size[2]\n",
    "        new_y_center = (y_center - bby1) / image_size[1]\n",
    "        new_width = width / image_size[2]\n",
    "        new_height = height / image_size[1]\n",
    "        new_labels.append([class_index, new_x_center, new_y_center, new_width, new_height])\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to implement CutMix data augmentation\n",
    "def cutmix_data(image_paths, labels_dict, output_folder, count, alpha=1.0):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for i in range(count):\n",
    "        # Randomly select two images\n",
    "        image_path_a, image_path_b = random.sample(image_paths, 2)\n",
    "\n",
    "        # Read images and labels\n",
    "        img_a = Image.open(image_path_a).convert('RGB')\n",
    "        img_b = Image.open(image_path_b).convert('RGB')\n",
    "        labels_a = labels_dict[image_path_a]\n",
    "        labels_b = labels_dict[image_path_b]\n",
    "\n",
    "        # Convert images to tensors\n",
    "        tensor_transform = transforms.ToTensor()\n",
    "        img_tensor_a = tensor_transform(img_a)\n",
    "        img_tensor_b = tensor_transform(img_b)\n",
    "\n",
    "        # Generate a random value from Beta distribution\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "\n",
    "        # Calculate the bounding box\n",
    "        bbx1, bby1, bbx2, bby2 = rand_bbox(img_tensor_a.size(), lam)\n",
    "\n",
    "        mixed_img_tensor = img_tensor_a.clone()\n",
    "        mixed_img_tensor[:, bby1:bby2, bbx1:bbx2] = img_tensor_b[:, bby1:bby2, bbx1:bbx2].clone()\n",
    "\n",
    "        # Update labels\n",
    "        mixed_labels = update_labels(labels_a, labels_b, bbx1, bby1, bbx2, bby2, img_tensor_a.size())\n",
    "\n",
    "        # Save the mixed image\n",
    "        output_image_path = os.path.join(output_folder, f\"{os.path.basename(image_path_a).replace('.jpg', '')}_{i}.jpg\")\n",
    "        mixed_img = transforms.ToPILImage()(mixed_img_tensor)\n",
    "        mixed_img.save(output_image_path)\n",
    "\n",
    "        # Save the corresponding labels\n",
    "        mixed_label_path = os.path.join(output_folder, f\"{os.path.basename(image_path_a).replace('.jpg', '')}_{i}.txt\")\n",
    "        with open(mixed_label_path, 'w') as f:\n",
    "            for label in mixed_labels:\n",
    "                label_str = ' '.join([str(elem) for elem in label])\n",
    "                f.write(label_str + '\\n')\n",
    "\n",
    "        print(f\"Processed and saved: {output_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load images and labels\n",
    "def load_images_and_labels(folder):\n",
    "    image_paths = []\n",
    "    labels_dict = {}\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith('jpg'):\n",
    "            image_path = os.path.join(folder, file)\n",
    "            image_paths.append(image_path)\n",
    "            label_path = image_path.replace('.jpg', '.txt')\n",
    "            with open(label_path, 'r') as f:\n",
    "                labels = []\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    labels.append([int(parts[0]), float(parts[1]), float(parts[2]), float(parts[3]), float(parts[4])])\n",
    "                labels_dict[image_path] = labels\n",
    "    return image_paths, labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: rawdata_cutmix\\frame_90420_0.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_6420_1.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_91980_2.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_55680_3.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_115020_4.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_89040_5.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_82800_6.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_127440_7.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_82500_8.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_84900_9.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_56460_10.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_86220_11.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_59460_12.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_123300_13.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_1020_14.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_159900_15.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_96540_16.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_42060_17.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_55560_18.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_169260_19.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_76200_20.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_89340_21.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_109140_22.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_37860_23.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_62460_24.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_87720_25.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_79740_26.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_82080_27.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_6960_28.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_34020_29.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_1020_30.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_91080_31.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_18840_32.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_62460_33.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_83160_34.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_84660_35.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_123240_36.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_37620_37.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_5160_38.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_82980_39.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_44520_40.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_92340_41.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_70920_42.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_80400_43.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_38520_44.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_76080_45.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_94980_46.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_81600_47.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_61920_48.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_124860_49.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_115140_50.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_3540_51.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_97920_52.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_135720_53.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_68520_54.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_83280_55.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_82980_56.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_73800_57.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_84780_58.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_122040_59.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_2640_60.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_21060_61.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_136680_62.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_54060_63.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_91980_64.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_61860_65.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_39420_66.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_159900_67.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_16500_68.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_123240_69.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_25380_70.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_8760_71.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_97800_72.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_26640_73.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_23100_74.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_83640_75.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_62460_76.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_82500_77.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_83340_78.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_68460_79.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_31680_80.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_24000_81.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_96780_82.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_26040_83.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_7980_84.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_1680_85.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_22080_86.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_16080_87.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_153360_88.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_2760_89.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_45720_90.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_94740_91.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_78420_92.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_87900_93.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_1260_94.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_66660_95.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_135720_96.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_61320_97.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_74820_98.jpg\n",
      "Processed and saved: rawdata_cutmix\\frame_16020_99.jpg\n"
     ]
    }
   ],
   "source": [
    "# set paths\n",
    "input_folder = 'raw_dataset_2'\n",
    "output_folder = \"rawdata_cutmix\"\n",
    "\n",
    "# Load images and labels\n",
    "image_paths, labels_dict = load_images_and_labels(input_folder)\n",
    "\n",
    "# Total number of images to generate\n",
    "total_images_to_generate = 100\n",
    "\n",
    "# Ensure there are at least two images for CutMix\n",
    "if len(image_paths) < 2:\n",
    "    raise ValueError(\"At least two images are required for CutMix.\")\n",
    "\n",
    "# Generate CutMix images\n",
    "cutmix_data(image_paths, labels_dict, output_folder, total_images_to_generate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tku_cutmix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
